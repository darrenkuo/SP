<div class="p"><!----></div>
<b>Topic:</b> Recursion and iteration</br></br>
<div class="p"><!----></div>
<b>Reading:</b>
<a href="http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-11.html#%_sec_1.2" target="_blank">Abelson & Sussman, Section 1.2 through 1.2.4</a>
<br/><br/>
<div class="p"><!----></div>
This week is about efficiency.  Mostly in 61A we don't care about that;
it becomes a focus of attention in 61B.  In 61A we're happy if you can
get a program working at all, except for this week, when we introduce
ideas that will be more important to you later.

<div class="p"><!----></div>
We want to know about the efficiency of algorithms, not of computer
hardware.  So instead of measuring runtime in microseconds or whatever,
we ask about the number of times some primitive (fixed-time) operation
is performed.  Example:

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/growth.scm
(define (square x) (* x x))

<div class="p"><!----></div>
(define (squares sent)
  (if (empty? sent)
      '()
      (se (square (first sent))
          (squares (bf sent)) )))

</pre></tt>

<div class="p"><!----></div>
To estimate the efficiency of this algorithm, we can ask, "if the argument
has N numbers in it, how many multiplications do we perform?"  The answer
is that we do one multiplication for each number in the argument, so we do
N altogether.  The amount of time needed should roughly double if the
number of numbers doubles.

<div class="p"><!----></div>
Another example:

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/growth.scm
(define (sort sent)
  (if (empty? sent)
      '()
      (insert (first sent)
              (sort (bf sent)) )))

<div class="p"><!----></div>
(define (insert num sent)
  (cond ((empty? sent) (se num sent))
        ((&lt; num (first sent)) (se num sent))
        (else (se (first sent) (insert num (bf sent)))) ))

</pre></tt>

<div class="p"><!----></div>
Here we are sorting a bunch of numbers by comparing them against each
other.  If there are N numbers, how many comparisons do we do?

<div class="p"><!----></div>
Well, if there are K numbers in the argument to <tt>insert</tt>, how
many comparisons does it do?  K of them.  How many times do we call <tt>
insert</tt>?  N times.  But it's a little tricky because each call to <tt>
insert</tt> has a different length sentence.  The range is from 0 to N<font face="symbol">-</font
>1.
So the total number of comparisons is actually

<br clear="all" /><table border="0" width="100%"><tr><td>
<table align="center" cellspacing="0"  cellpadding="2"><tr><td nowrap="nowrap" align="center">
0+1+2+<font face="symbol"></font
>+(N<font face="symbol">-</font
>2)+(N<font face="symbol">-</font
>1)</td></tr></table>
</td></tr></table>


which turns out to be <sup>1</sup>/<sub>2</sub>N(N<font face="symbol">-</font
>1).  For large N, this is roughly
equal to <sup>1</sup>/<sub>2</sub>N<sup>2</sup>.  If the number of numbers doubles, the time
required should quadruple.

<div class="p"><!----></div>
That constant factor of [1/2] isn't really very important, since we
don't really know what we're halving-that is, we don't know exactly how
long it takes to do one comparison.  If we want a very precise measure of
how many microseconds something will take, then we have to worry about the
constant factors, but for an overall sense of the nature of the algorithm,
what counts is the N<sup>2</sup> part.  If we double the size of the input to a
program, how does that affect the running time?

<div class="p"><!----></div>
We use "big O" and "big Theta" notation to express this sort of
approximation.  We say that the running time of the <tt>sort</tt> function is
<font face="symbol">Q</font
>(N<sup>2</sup>) while the running time of the <tt>squares</tt> function is
<font face="symbol">Q</font
>(N).  The formal definition is

<div class="p"><!----></div>

<br clear="all" /><table border="0" width="100%"><tr><td>
<table border="0" cellspacing="0" cellpadding="0">
 <tr><td width="50%"></td><td nowrap="nowrap" align="right" colspan="1"><table border="0" cellspacing="0" cellpadding="2"><tr><td nowrap="nowrap" align="left">
f  <font face="symbol"></font
> O(g) </td></tr></table></td><td nowrap="nowrap" align="left">
<table border="0" cellspacing="0" cellpadding="2"><tr><td nowrap="nowrap" align="left">
<font face="symbol"></font
> <font face="symbol">$</font
>k,N &gt; 0  | <font face="symbol">"</font
>x &gt; N, <font face="symbol">|</font
>f(x)<font face="symbol">|</font
> <font face="symbol"></font
> k&#183;<font face="symbol">|</font
>g(x)<font face="symbol">|</font
></td></tr></table></td><td width="50%"></td></tr>
 <tr><td width="50%"></td><td nowrap="nowrap" align="right" colspan="1"><table border="0" cellspacing="0" cellpadding="2"><tr><td nowrap="nowrap" align="left">
f  <font face="symbol"></font
> <font face="symbol">Q</font
>(g) </td></tr></table></td><td nowrap="nowrap" align="left">
<table><tr><td nowrap="nowrap" align="right" colspan="1"><font face="symbol"></font
> f <font face="symbol"></font
> O(g) <font face="symbol"></font
>g <font face="symbol"></font
> O(f)</td></tr></table></td></tr></table>
</td></tr></table>



<div class="p"><!----></div>
What does all this mean?  Basically O means that one function is always less
than another function (e.g., the time for your program to run is less than
x<sup>2</sup>) except that we don't care about constant factors (that's what the k
means) and we don't care about small values of x (that's what the N
means).  <font face="symbol">Q</font
> means that each of two functions is within a constant
factor of the other.  (Technically <font face="symbol">Q</font
>(g) is the set of all functions
that bear this relationship to g, and f <font face="symbol"></font
> <font face="symbol">Q</font
>(g) means that f is an
element of that set of functions.  And of course instead of <font face="symbol">Q</font
>(N<sup>2</sup>)
we should really write <font face="symbol">Q</font
>(N<font face="symbol"></font
> N<sup>2</sup>), but nobody does.)

<div class="p"><!----></div>
Why don't we care about small values of x?  Because for small inputs, your
program will be fast enough anyway.  Let's say one program is 1000 times
faster than another, but one takes a millisecond and the other takes a
second.  Big deal.

<div class="p"><!----></div>
Why don't we care about constant factors?  Because for large inputs, the
constant factor will be drowned out by the order of growth-the exponent
in the <font face="symbol">Q</font
>(x<sup>i</sup>) notation.  Here is an example taken from the book <i>
Programming Pearls </i> by Jon Bentley (Addison-Wesley, 1986).  He ran
two different programs to solve the same problem.  One was a fine-tuned
program running on a Cray supercomputer, but using an <font face="symbol">Q</font
>(N<sup>3</sup>) algorithm.
The other algorithm was run on a Radio Shack microcomputer, so its
constant factor was several million times bigger, but the algorithm
was <font face="symbol">Q</font
>(N).  For small N the Cray was much faster, but for small N both
computers solved the problem in less than a minute.  When N was large
enough for the problem to take a few minutes or longer, the Radio Shack
computer's algorithm was faster.

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/bentley

<div class="p"><!----></div>
                t1(N) = 3.0 N<sup>3</sup>          t2(N) = 19,500,000 N

<div class="p"><!----></div>
        N       CRAY-1 Fortran          TRS-80 Basic

<div class="p"><!----></div>
        10      3.0 microsec            200 millisec
       100      3.0 millisec            2.0 sec
      1000      3.0 sec                 20 sec
     10000      49 min                  3.2 min
    100000      35 days                 32 min
   1000000      95 yrs                  5.4 hrs

</pre></tt>

<div class="p"><!----></div>
Typically, the algorithms you run across can be grouped into four categories
according to their order of growth in time required.  The first category is
<i>searching </i> for a particular value out of a collection of values,
e.g., finding someone's telephone number.  The most obvious algorithm (just
look through all the values until you find the one you want) is <font face="symbol">Q</font
>(N) time,
but there are smarter algorithms that can work in <font face="symbol">Q</font
>(logN) time or even
in <font face="symbol">Q</font
>(1) (that is, constant) time.  The second category is <i>sorting </i>
a bunch of values into some standard order.  (Many other problems that are
not explicitly about sorting turn out to require similar approaches.)  The
obvious sorting algorithms are <font face="symbol">Q</font
>(N<sup>2</sup>) and the clever ones are <font face="symbol">Q</font
>(N logN).  A third category includes relatively obscure problems such as matrix
multiplication, requiring <font face="symbol">Q</font
>(N<sup>3</sup>) time.  Then there is an enormous jump to
the really hard problems that require <font face="symbol">Q</font
>(2<sup>N</sup>) or even <font face="symbol">Q</font
>(N!) time; these
problems are effectively not solvable for values of N greater than one or
two dozen.  (Inventing faster computers won't help; if the speed of your
computer doubles, that just adds 1 to the largest problem size you can
handle!)  Trying to find faster algorithms for these <i>intractable </i>
problems is a current hot research topic in computer science.

<div class="p"><!----></div>

<div class="p"><!----></div>
<font face="symbol"></font
> Iterative processes

<div class="p"><!----></div>
So far we've been talking about time efficiency, but there is also
memory (space) efficiency.  This has gotten less important as memory
has gotten cheaper, but it's still somewhat relevant because using a lot of
memory increases swapping (not everything fits at once) and so indirectly
takes time.

<div class="p"><!----></div>
The immediate issue for today is the difference between a <i>linear
recursive process </i> and an <i>iterative process</i>.

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/count.scm
(define (count sent)
  (if (empty? sent)
      0
      (+ 1 (count (bf sent))) ))

</pre></tt>

<div class="p"><!----></div>
This function counts the number of words in a sentence.  It takes <font face="symbol">Q</font
>(N)
time.  It also requires <font face="symbol">Q</font
>(N) space, not counting the space for the
sentence itself, because Scheme has to keep track of N pending
computations during the processing:

<div class="p"><!----></div>
<tt>  <pre>(count '(i want to hold your hand))
(+ 1 (count '(want to hold your hand)))
(+ 1 (+ 1 (count '(to hold your hand))))
(+ 1 (+ 1 (+ 1 (count '(hold your hand)))))
(+ 1 (+ 1 (+ 1 (+ 1 (count '(your hand))))))
(+ 1 (+ 1 (+ 1 (+ 1 (+ 1 (count '(hand)))))))
(+ 1 (+ 1 (+ 1 (+ 1 (+ 1 (+ 1 (count '())))))))
(+ 1 (+ 1 (+ 1 (+ 1 (+ 1 (+ 1 0))))))
(+ 1 (+ 1 (+ 1 (+ 1 (+ 1 1)))))
(+ 1 (+ 1 (+ 1 (+ 1 2))))
(+ 1 (+ 1 (+ 1 3)))
(+ 1 (+ 1 4))
(+ 1 5)
6

</pre></tt>

<div class="p"><!----></div>
When we get halfway through this chart and compute <tt>(count&nbsp;'())</tt>,
we aren't finished with the entire problem.  We have to remember to
add 1 to the result six times.  Each of those remembered tasks requires some
space in memory until it's finished.

<div class="p"><!----></div>
Here is a more complicated program that does the same thing differently:

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/count.scm
(define (count sent)
  (define (iter wds result)
    (if (empty? wds)
        result
        (iter (bf wds) (+ result 1)) ))
  (iter sent 0) )

</pre></tt>

<div class="p"><!----></div>
This time, we don't have to remember uncompleted tasks; when we reach
the base case of the recursion, we have the answer to the entire
problem:

<div class="p"><!----></div>
<tt>  <pre>(count '(i want to hold your hand))
(iter '(i want to hold your hand) 0)
(iter '(want to hold your hand) 1)
(iter '(to hold your hand) 2)
(iter '(hold your hand) 3)
(iter '(your hand) 4)
(iter '(hand) 5)
(iter '() 6)
6

</pre></tt>

<div class="p"><!----></div>

<div class="p"><!----></div>
When a process has this structure, Scheme does not need extra memory
to remember all the unfinished tasks during the computation.

<div class="p"><!----></div>
This is really not a big deal.  For the purposes of this course, you should
generally use the simpler linear-recursive structure and not try for the
more complicated iterative structure; the efficiency saving is not worth the
increased complexity.  The reason Abelson and Sussman make a fuss about it
is that in other programming languages any program that is recursive in <i>
form </i> (i.e., in which a function invokes itself) will take (at least)
linear space even if it could theoretically be done iteratively.  These
other languages have special iterative syntax (<tt>for</tt>, <tt>while</tt>, and
so on) to avoid recursion.  In Scheme you can use the function-calling
mechanism and still achieve an iterative process.

<div class="p"><!----></div>
<font face="symbol"></font
> More is less: non-obvious efficiency improvements.

<div class="p"><!----></div>
The nth row of Pascal's triangle contains the constant coefficients of the
terms of (a+b)<sup>n</sup>.  Each number in Pascal's triangle is the sum of the two
numbers above it.  So we can write a function to compute these numbers:

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/pascal.scm
(define (pascal row col)
  (cond ((= col 0) 1)
        ((= col row) 1)
        (else (+ (pascal (- row 1) (- col 1))
                 (pascal (- row 1) col) ))))

</pre></tt>

<div class="p"><!----></div>
This program is very simple, but it takes <font face="symbol">Q</font
>(2<sup>n</sup>) time!  [Try some
examples.  Row 18 is already getting slow.]

<div class="p"><!----></div>
Instead we can write a more complicated program that, on the surface,
does a lot more work because it computes an <i>entire row </i> at a time
instead of just the number we need:

<div class="p"><!----></div>
<tt>  <pre>;;;;;                        In file cs61a/lectures/1.2/pascal.scm
(define (new-pascal row col)
  (nth col (pascal-row row)) )

<div class="p"><!----></div>
(define (pascal-row row-num)
  (define (iter in out)
    (if (empty? (bf in))
        out
        (iter (bf in) (se (+ (first in) (first (bf in))) out)) ))
  (define (next-row old-row num)
    (if (= num 0)
        old-row
        (next-row (se 1 (iter old-row '(1))) (- num 1)) ))
  (next-row '(1) row-num) )

</pre></tt>

<div class="p"><!----></div>
This was harder to write, and seems to work harder, but it's incredibly
faster because it's <font face="symbol">Q</font
>(N<sup>2</sup>).

<div class="p"><!----></div>
The reason is that the original version computed lots of entries repeatedly.
The new version computes a few unnecessary ones, but it only computes each
entry once.

<div class="p"><!----></div>
Moral:  When it really matters, think hard about your algorithm instead
of trying to fine-tune a few microseconds off the obvious algorithm.

<div class="p"><!----></div>
<b>Note:</b> Programming project 1 is assigned this week.

<div class="p"><!----></div>

<div class="p"><!----></div>

<div class="p"><!----></div>

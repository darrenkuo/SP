<b>CS 61A&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Lecture Notes&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp; Week 13</b>
<p>
Topic: Analyzing evaluator, MapReduce
<p>
<b>Reading:</b>
Abelson &amp; Sussman, Section
<a href="http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1.7http://mitpress.mit.edu/sicp/full-text/book/book-Z-H-26.html#%_sec_4.1.7">4.1.7</a>,
<a href="http://inst.eecs.berkeley.edu/~cs61as/reader/mapreduce-osdi04.pdf">MapReduce paper</a>
<p>

To work with the ideas in this section you should first

<p>
<tt>  <pre>(load "&#126;cs61a/lib/analyze.scm")

</pre></tt>
in order to get the analyzing metacircular evaluator.

<p>
<b>Inefficiency in the Metacircular Evaluator</b>

<p>
Suppose we've defined the factorial function as follows:

<p>
<tt>  <pre>(define (fact num)
  (if (= num 0)
      1
      (* num (fact (- num 1)))))

</pre></tt>

<p>
What happens when we compute <tt>(fact&nbsp;3)</tt>?

<p>
<tt>  <pre>eval (fact 3)
  self-evaluating?  ==&#62;  #f     if-alternative  ==&#62;  (* num (fact (- num 1)))
  variable?  ==&#62;  #f               eval (* num (fact (- num 1)))
  quoted?  ==&#62;  #f                   self-evaluating?  ==&#62;  #f
  assignment?  ==&#62;  #f               ...
  definition?  ==&#62;  #f               list-of-values (num (fact (- num 1)))
  if?  ==&#62;  #f                         ...
  lambda?  ==&#62;  #f                     eval (fact (- num 1))
  begin?  ==&#62;  #f                        ...
  cond?  ==&#62;  #f                         apply &lt;procedure fact&#62; (2)
  application?  ==&#62;  #t                    eval (if (= num 0) ...)
  eval fact
    self-evaluating?  ==&#62;  #f
    variable?  ==&#62;  #t
    lookup-variable-value  ==&#62;  &lt;procedure fact&#62;
    list-of-values (3)
      eval 3  ==&#62; 3
    apply &lt;procedure fact&#62; (3)
      eval (if (= num 0) ...)
	self-evaluating?  ==&#62;  #f
	variable?  ==&#62;  #f
	quoted?  ==&#62;  #f
	assignment?  ==&#62;  #f
	definition?  ==&#62;  #f
        if?  ==&#62;  #t
          eval-if (if (= num 0) ...)
            if-predicate  ==&#62;  (= num 0)
              eval (= num 0)
                self-evaluating?  ==&#62;  #f
                ...

</pre></tt>

<p>
Four separate times, the evaluator has to examine the procedure body,
decide that it's an <tt>if</tt> expression, pull out its component parts,
and evaluate those parts (which in turn involves deciding what type
of expression each part is).

<p>
This is one reason why interpreted languages are so much slower than compiled
languages:  The interpreter does the syntactic analysis of the program over
and over again.  The compiler does the analysis once, and the compiled
program can just do the part of the computation that depends on the actual
values of variables.

<p>
<b>Separating Analysis from Execution</b>

<p>
<tt>Eval</tt> takes two arguments, an expression and an environment.  Of those,
the expression argument is (obviously!)&nbsp;the same every time we revisit the
same expression, whereas the environment will be different each time.  For
example, when we compute <tt>(fact&nbsp;3)</tt> we evaluate the body of <tt>fact</tt>
in an environment in which <tt>num</tt> has the value 3.  That body includes
a recursive call to compute <tt>(fact&nbsp;2)</tt>, in which we evaluate the same
body, but now in an environment with <tt>num</tt> bound to 2.

<p>
Our plan is to look at the evaluation process, find those parts which depend
only on <tt>exp</tt> and not on <tt>env</tt>, and do those only once.  The
procedure that does this work is called <tt>analyze</tt>.

<p>
What is the result of <tt>analyze</tt>?  It has to be something that can
be combined somehow with an environment in order to return a value.
The solution is that <tt>analyze</tt> returns a procedure that takes only
<tt>env</tt> as an argument, and does the rest of the evaluation.

<p>
Instead of

<p>
<tt>  <pre>(eval exp env)  ==&#62;  value

</pre></tt>

<p>
we now have

<p>
<tt>  <pre>1.  (analyze exp)  ==&#62;  exp-procedure
2.  (exp-procedure env)  ==&#62;  value

</pre></tt>

<p>
When we evaluate the same expression again, we only have to repeat step 2.
What we're doing is akin to memoization, in that we remember the result
of a computation to avoid having to repeat it.  The difference is that
now we're remembering something that's only part of the solution to the
overall problem, instead of a complete solution.

<p>
We can duplicate the effect of the original <tt>eval</tt> this way:

<p>
<tt>  <pre>(define (eval exp env)
  ((analyze exp) env))

</pre></tt>

<p>
<b>The Implementation Details</b>

<p>
<tt>Analyze</tt> has a structure similar to that of the original eval:

<p>
<tt>  <pre>(define (eval exp env)                (define (analyze exp)
  (cond ((self-evaluating? exp)         (cond ((self-evaluating? exp)
         exp)                                  (analyze-self-eval exp))
        ((variable? exp)                      ((variable? exp)
         (lookup-var-val exp env))             (analyze-var exp))
        ...                                   ...
        ((foo? exp) (eval-foo exp env))       ((foo? exp) (analyze-foo exp))
        ...))                                 ...))

</pre></tt>

<p>
The difference is that the procedures such as <tt>eval-if</tt> that take
an expression and an environment as arguments have been replaced by
procedures such as <tt>analyze-if</tt> that take only the expression as argument.

<p>

<p>
How do these analysis procedures work?  As an intermediate step in our
understanding, here is a version of <tt>analyze-if</tt> that exactly follows
the structure of <tt>eval-if</tt> and doesn't save any time:

<p>
<tt>  <pre>(define (eval-if exp env)
  (if (true? (eval (if-predicate exp) env))
      (eval (if-consequent exp) env)
      (eval (if-alternative exp) env)))

(define (analyze-if exp)
  (lambda (env)
    (if (true? (eval (if-predicate exp) env))
	(eval (if-consequent exp) env)
	(eval (if-alternative exp) env))))

</pre></tt>

<p>
This version of <tt>analyze-if</tt> returns a procedure with <tt>env</tt> as its
argument, whose body is exactly the same as the body of the original
<tt>eval-if</tt>.  Therefore, if we do

<p>
<tt>  <pre>((analyze-if some-if-expression) some-environment)

</pre></tt>

<p>
the result will be the same as if we'd said

<p>
<tt>  <pre>(eval-if some-if-expression some-environment)

</pre></tt>

<p>
in the original metacircular evaluator.

<p>
But we'd like to improve on this first version of <tt>analyze-if</tt> because
it doesn't really avoid any work.  Each time we call the procedure that
<tt>analyze-if</tt> returns, it will do all of the work that the original
<tt>eval-if</tt> did.

<p>
The first version of <tt>analyze-if</tt> contains three calls to <tt>eval</tt>.
Each of those calls does an analysis of an expression and then a
computation of the value in the given environment.  What we'd like to
do is split each of those <tt>eval</tt> calls into its two separate parts,
and do the first part only once, not every time:

<p>
<tt>  <pre>(define (analyze-if exp)
  (let ((pproc (analyze (if-predicate exp)))
	(cproc (analyze (if-consequent exp)))
	(aproc (analyze (if-alternative exp))))
    (lambda (env)
      (if (true? (pproc env))
	  (cproc env)
	  (aproc env)))))

</pre></tt>

<p>
In this final version, the procedure returned by <tt>analyze-if</tt>
doesn't contain any analysis steps.  All of the components were
already analyzed before we call that procedure, so no further
analysis is needed.

<p>
The biggest gain in efficiency comes from the way in which <tt>lambda</tt>
expressions are handled.  In the original metacircular evaluator,
leaving out some of the data abstraction for clarity here, we have

<p>
<tt>  <pre>(define (eval-lambda exp env)
  (list 'procedure exp env))

</pre></tt>

<p>
The evaluator does essentially nothing for a <tt>lambda</tt> expression
except to remember the procedure's text and the environment in which
it was created.  But in the analyzing evaluator we analyze the body
of the procedure; what is stored as the representation of the procedure
does not include its text!  Instead, the evaluator represents a procedure
in the metacircular Scheme as a procedure in the underlying Scheme,
along with the formal parameters and the defining environment.

<p>

<p>
<b>Level Confusion</b>

<p>
The analyzing evaluator turns an expression such as

<p>
<tt>  <pre>(if A B C)

</pre></tt>

<p>
into a procedure

<p>
<tt>  <pre>(lambda (env)
  (if (A-execution-procedure env)
      (B-execution-procedure env)
      (C-execution-procedure env)))

</pre></tt>

<p>
This may seem like a step backward; we're trying to implement <tt>if</tt>
and we end up with a procedure that does an <tt>if</tt>.  Isn't this
an infinite regress?

<p>
No, it isn't.  The <tt>if</tt> in the execution procedure is handled by the
underlying Scheme, not by the metacircular Scheme.  Therefore, there's no
regress; we don't call <tt>analyze-if</tt> for that one.  Also, the <tt>if</tt> in
the underlying Scheme is much faster than having to do the syntactic
analysis for the <tt>if</tt> in the meta-Scheme.

<p>
<b>So What?</b>

<p>
The syntactic analysis of expressions is a large part of what a compiler
does.  In a sense, this analyzing evaluator is a compiler!  It compiles
Scheme into Scheme, so it's not a very useful compiler, but it's really
not that much harder to compile into something else, such as the machine
language of a particular computer.

<p>
A compiler whose structure is similar to this one is called a <i>recursive
descent </i> compiler.  Today, in practice, most compilers use a different
technique (called a stack machine) because it's possible to automate the
writing of a parser that way.  (I mentioned this earlier as an example of
data-directed programming.)  But if you're writing a parser by hand, it's
easiest to use recursive descent.

<p>

<p>
<font face="symbol"></font
> <b>Software reliability: the Therac failures</b>

<p>
<br /><br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
6 accidents, 4 deaths
</dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
but 100s of lives saved
</dd></dl></dd></dl><br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
no bad guys (cf. Ford Pinto case)
</dd></dl><br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
Software doesn't degrade like hardware
</dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
but it rots anyway
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
but it has much greater complexity
</dd></dl></dd></dl><dl compact="compact"><dt></dt>
<dd>cf. Star Wars (birth of CPSR)
</dd></dl><br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
Continuum of life-or-deathness: Clearly Therac yes, clearly video game no.
</dd></dl> But what about OS, spreadsheet, etc.?

<p>
<br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
Therac bugs
</dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
no atomic test and set
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
hardware interlocks removed
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
UI problems:
</dd></dl></dd></dl> <dl><dd>cursor position
</dd></dl>
 <dl><dd>defaults
</dd></dl>
 <dl><dd>too many error messages
</dd></dl>
<dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
documentation
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
organizational response
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt></dt><dd>
easy to see after the fact, but problems are inherent in
organizations (esp. ones that can be sued)
</dd></dl></dd></dl><br /><br />
<dl compact="compact">
 <dt>
<font face="symbol"></font
></dt>
<dd>
Solutions
</dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
redundancy
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
fail soft (work despite bugs)
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
audit trail
</dd></dl></dd></dl><dl><dd><dl compact="compact"><dt>\triangleright</dt><dd>
Software Engineering (an attitude about programming)
</dd></dl></dd></dl> <dl><dd>Design techniques
</dd></dl>
 <dl><dd>modularization (cf. OOP)
</dd></dl>
 <dl><dd>understand concurrency (semaphores)
</dd></dl>
 <dl><dd>analyze invariants
</dd></dl>
 <dl><dd>Verification techniques
</dd></dl>
 <dl><dd>correctness proofs</dd></dl>
(can't be perfect because of halting theorem but still useful)

<p>
 <dl><dd>automatic analysis in compiler
</dd></dl>
 <dl><dd>Debugging techniques
</dd></dl>
 <dl><dd>black box vs. glass box
</dd></dl>
 <dl><dd>don't break old code with new fix
</dd></dl>
 <dl><dd>introduce bugs on purpose to analyze results downstream
</dd></dl>
 <dl><dd>debug by subtraction, not addition
</dd></dl>
<b>Note: The first part of programming project 4 is this week.</b>

<p>

<p>
<P><b>&bull; Mapreduce part 2</b>

<p>
Here's the diagram of mapreduce again:

<p>

<table align="center" border="0"><tr><td>
<a href="mr.eps">Figure</a></td></tr></table><!--hboxt-->

<p>
The seemingly unpoetic names <tt>f1</tt> and <tt>f2</tt> serve to remind you of two
things: <tt>f1</tt> (the mapper) is used before <tt>f2</tt> (the reducer), and <tt>
f1</tt> takes one argument while <tt>f2</tt> takes two arguments (just like the
functions used with ordinary <tt>map</tt> and <tt>accumulate</tt> respectively).

<p>

<table align ="left" border="0" width="100%"><tr><td>
mapper:&nbsp;&nbsp;<tt>kv-pair</tt>&nbsp;&nbsp;<font face="symbol"></font
>&nbsp;&nbsp;<tt>
list-of-kv-pairs</tt></td><td align="right"></td></tr></table><!--hbox-->
<br clear="all" />
<p>
<table align ="left" border="0" width="100%"><tr><td>
reducer:&nbsp;&nbsp;<tt>value</tt>, <tt>partial-result</tt>&nbsp;&nbsp;<font face="symbol"></font
>&nbsp;&nbsp;<tt>
result</tt></td><td align="right"></td></tr></table><!--hbox-->
<br clear="all" /><b>All data are in the form of key-value pairs.</b> Ordinary <tt>map</tt> doesn't
care what the elements of the data list argument are, but <tt>mapreduce</tt>
works only with data each of which is a key-value pair.  In the Scheme
interface to the distributed filesystem, a file is a stream.  Every element of
the file stream represents one line of the file, using a key-value pair whose
key is the filename and whose value is a list of words, representing the text
of the line.  The <tt>cdr</tt> of a file stream is a promise to ask the
distributed filesystem for the next line.

<p>
<b>Each processor runs a separate <tt>stream-map</tt>.</b>  The overlapping squares
at the left of the mapreduce picture represent an entire stream.  (How is a
large distributed file divided among map processes?  It doesn't really matter,
as far as the <tt>mapreduce</tt> user is concerned; <tt>mapreduce</tt> tries to do
it as efficiently as possible given the number of processes and the location
of the data in the filesystem.)  The entire stream is the input to a <tt>map</tt>
process; each element of the stream (a kv-pair) is the input to your mapper
function <tt>f1</tt>.

<p>
<b>For each key-value pair in the input stream, the mapper returns a
<u>list</u> of key-value pairs.</b> In the simplest case, each of these lists
will have one element; the code will look something like

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (list (make-kv-pair ... ...)))

</pre></tt>

<p>
The interface requires that you return a list to allow for the non-simplest
cases: (1) Each input key-value pair may give rise to more than one output
key-value pair.  For example, you may want an output key-value pair for each
<i>word </i> of the input file, whereas the input key-value pair represents an
entire line:

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (map (lambda (wd) (make-kv-pair ... ...))
       (kv-value input-kv-pair)))

</pre></tt>

<p>
(2) There are <i>three </i> commonly used higher order functions
for sequential data, <tt>map</tt>, <tt>accumulate</tt>/<tt>reduce</tt>, and <tt>
filter</tt>.  The way <tt>mapreduce</tt> handles the sort of problem for which <tt>
filter</tt> would ordinarily be used is to allow a mapper to return an empty list
if this particular key-value pair shouldn't contribute to the result:

<p>
<tt>  <pre>(define (my-mapper input-kv-pair)
  (if ...
      (list input-kv-pair)
      '()))

</pre></tt>

<p>
Of course it's possible to write mapper functions that combine these three
patterns for more complicated tasks.

<p>
The keys in the kv-pairs returned by the mapper need not be the same as the
key in the input kv-pair.

<p>
<b>Instead of one big accumulation, there's a separate accumulation of
values for each key.</b> The non-parallel computation in the left half of the
picture has two steps, a <tt>map</tt> and an <tt>accumulate</tt>.  But the <tt>
mapreduce</tt> computation has <i>three </i> steps; the middle step sorts all the
key-value pairs produced by all the mapper processes by their keys, and
combines all the kv-pairs with the same key into a single aggregate structure,
which is then used as the input to a <tt>reduce</tt> process.

<p>
<i>This is why the use of key-value pairs is important! </i> If the data had
no such structure imposed on them, there would be no way for us to tell
<tt>mapreduce</tt> which data should be combined in each reduction.

<p>
Although it's shown as one big box, the sort is also done in parallel; it's a
"bucket sort," in which each <tt>map</tt> process is responsible for sending
each of its output kv-pairs to the proper <tt>reduce</tt> process.  (Don't be
confused; your mapper function doesn't have to do that.  The <tt>mapreduce</tt>
program takes care of it.)

<p>
Since all the data seen by a single <tt>reduce</tt> process have the same key,
the reducer doesn't deal with keys at all.  This is important because it
allows us to use simple reducer functions such as <tt>+</tt>, <tt>*</tt>, <tt>max</tt>,
etc.  The Scheme interface to <tt>mapreduce</tt> recognizes the special cases of
<tt>cons</tt> and <tt>cons-stream</tt> as reducers and does what you intend, even
though it wouldn't actually work without this special handling, both because
<tt>cons-stream</tt> is a special form and because the iterative implementation
of <tt>mapreduce</tt> would do the combining in the wrong order.

<p>
In the underlying <tt>mapreduce</tt> software, each <tt>reduce</tt> process leaves
its results in a separate file, stored on the particular processor that ran
the process.  But the Scheme interface to <tt>mapreduce</tt> returns a single
value, a stream that effectively merges the results from all the
<tt>reduce</tt> processes.

<p>

<p>
<b>Running mapreduce:</b> The <tt>mapreduce</tt> function is not available on the
standard lab machines.  You must connect to the machine that controls the
parallel cluster.  To do this, from the Unix shell you say this:

<p>
<tt>  <pre>ssh icluster1.eecs.berkeley.edu

</pre></tt>

<p>
If you're at home, rather than in the lab, you'll have to provide your class
login to the <tt>ssh</tt> command:

<p>
<tt>  <pre>ssh cs61a-XY@icluster.eecs.berkeley.edu

</pre></tt>

<p>
replacing <tt>XY</tt> above with your login account.  <tt>Ssh</tt> will ask for your
password, which is the same on the parallel cluster as for your regular class
account.  Once you are logged into <tt>icluster1</tt>, you can run <tt>stk</tt> as
usual, but <tt>mapreduce</tt> will be available:

<p>
<tt>  <pre>(mapreduce mapper reducer reducer-base-case filename-or-special-stream)

</pre></tt>

<p>
The first three arguments are the mapper function for the <tt>map</tt> phase,
and the reducer function and starting value for the <tt>reduce</tt> phase.  The
last argument is the data input to the <tt>map</tt>, but it is restricted to
be either a distributed filesystem folder, which must be one of these:

<p>
<tt>  <pre>"/beatles-songs"         <span class="roman">This one is small and has all Beatles song names</span><!--hbox-->
"/gutenberg/shakespeare" <span class="roman">The collected works of William Shakespeare</span><!--hbox-->
"/gutenberg/dickens"     <span class="roman">The collected works of Charles Dickens</span><!--hbox-->
"/sample-emails"         <span class="roman">Some sample email data for the homework</span><!--hbox-->
"/large-emails"          <span class="roman">A much larger sample email dataset. Use this only</span><!--hbox-->
                         <span class="roman">if you're willing to wait a while.</span><!--hbox-->

</pre></tt>

<p>
(the quotation marks above are required), or the stream returned by an earlier
call to <tt>mapreduce</tt>.  (Streams you make yourself with <tt>cons-stream</tt>,
etc., can't be used.)  Some problems are solved with two <tt>mapreduce</tt>
passes, like this:

<p>
<tt>  <pre>(define intermediate-result (mapreduce ...))
(mapreduce ... intermediate-result)

</pre></tt>

<p>
(Yes, you could just use one <tt>mapreduce</tt> call directly as the argument to
the second <tt>mapreduce</tt> call, but in practice you'll want to use
<tt>show-stream</tt> to examine the intermediate result first, to make sure the
first call did what you expect.)

<p>
Here's a sample.  We provide a file of key-value pairs in which the key is
the name of a Beatles album and the value is the name of a song on that album.
Suppose we want to know how many times each word appears in the name of a
song:

<p>
<tt>  <pre>(define (wordcount-mapper document-line-kv-pair)
  (map (lambda (wd-in-line) (make-kv-pair wd-in-line 1))
       (kv-value document-line-kv-pair)))

(define wordcounts (mapreduce wordcount-mapper + 0 "/beatles-songs"))

&#62; (ss wordcounts)

</pre></tt>

<p>
The argument to <tt>wordcount-mapper</tt> will be a key-value pair whose key is
an album name, and whose value is a song name.  (In other examples, the key
will be a filename, such as the name of a play by Shakespeare, and the value
will be a line from the play.)  We're interested only in the song names, so
there's no call to <tt>kv-key</tt> in the procedure.  For each song name, we
generate a list of key-value pairs in which the key is a word in the name and
the value is 1.  This may seem silly, having the same value in every pair,
but it means that in the <tt>reduce</tt> stage we can just use <tt>+</tt> as the
reducer, and it'll add up all the occurrences of each word.

<p>
You'll find the running time disappointing in this example; since the number
of Beatles songs is pretty small, the same computation could be done faster on
a single machine.  This is because there is a significant setup time both
for <tt>mapreduce</tt> itself and for the <tt>stk</tt> interface.  Since your mapper
and reducer functions have to work when run on parallel machines, your Scheme
environment must be shipped over to each of those machines before the
computation begins, so that bindings are available for any free references in
your procedures.  It's only for large amounts of data (or long computations
that aren't data-driven, such as calculating a trillion digits of <font face="symbol">p</font
>, but
<tt>mapreduce</tt> isn't really appropriate for those examples) that parallelism
pays off.

<p>
By the way, if you want to examine the input file, you can't just say

<p>
<tt>  <pre>(ss "/beatles-songs")	; NO

</pre></tt>

<p>
because a distributed filename isn't a stream, even though the file itself
is (when viewed by the <tt>stk</tt> interface to <tt>mapreduce</tt>) a stream.
These filenames only work as arguments to <tt>mapreduce</tt> itself.  But we
can use <tt>mapreduce</tt> to examine the file by applying null transformations
in the map and reduce stages:

<p>
<tt>  <pre>(ss (mapreduce list cons-stream the-empty-stream "/beatles-songs"))

</pre></tt>

<p>
The mapper function is <tt>list</tt> because the mapper must always return a
list of key-value pairs; in this case, <tt>map</tt> will call <tt>list</tt> with
one argument and so it'll return a list of length one.

<p>
Now we'd like to find the most commonly used word in Beatle song titles.
There are few enough words so that we could really do this on one processor,
but as an exercise in parallelism we'll do it partly in parallel.  The trick
is to have each reduce process find the most common word starting with a
particular letter.  Then we'll have 26 candidates from which to choose the
absolutely most common word on one processor.

<p>
<tt>  <pre>(define (find-max-mapper kv-pair)
  (list (make-kv-pair (first (kv-key kv-pair))
		      kv-pair)))

(define (find-max-reducer current so-far)
  (if (&#62; (kv-value current) (kv-value so-far))
      current
      so-far))

(define frequent (mapreduce find-max-mapper find-max-reducer
			    (make-kv-pair 'foo 0) wordcounts))

&#62; (ss frequent)

&#62; (stream-accumulate find-max-reducer (make-kv-pair 'foo 0)
		     (stream-map kv-value frequent))

</pre></tt>

<p>
This is a little tricky.  In the <tt>wordcounts</tt> stream, each key-value
pair has a word as the key, and the count for that word as the value:
<tt>(back&nbsp;.&nbsp;3)</tt>.  The mapper transforms this into a key-value pair in
which the key is the first letter of the word, and the value is <i>the
entire input key-value pair </i>: <tt>(b&nbsp;.&nbsp;(back&nbsp;.&nbsp;3))</tt>.  Each <tt>reduce</tt>
process gets all the pairs with a particular key, i.e., all the ones with
the same first letter of the word.  The reducer sees only the values from
those pairs, but each value is itself a key-value pair!  That's why the
reducer has to compare the <tt>kv-value</tt> of its two arguments.

<p>
As another example, here's a way to count the total number of lines in all
of Shakespeare's plays:

<p>
<tt>  <pre>(define will (mapreduce (lambda (kv-pair) (list (make-kv-pair 'line 1)))
			+ 0 "/gutenberg/shakespeare"))

</pre></tt>

<p>
For each line in Shakespeare, we make exactly the same pair <tt>(line&nbsp;.&nbsp;1)</tt>.
Then, in the <tt>reduce</tt> stage, all the ones in all those pairs are added.
But this is actually a bad example!  Since all the keys are the same (the
word <tt>line</tt>), only one <tt>reduce</tt> process is run, so the counting isn't
done in parallel.  A better way would be to count each play separately, then
add those results if desired.  You'll do that in lab.

<p>

<p>
